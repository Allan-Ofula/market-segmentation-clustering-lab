


# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Setting standard configurations
np.random.seed(0)
get_ipython().run_line_magic("matplotlib", " inline")





raw_df = pd.read_csv('wholesale_customers_data.csv')
raw_df.head()





channels = raw_df['Channel']
df = raw_df.drop(['Channel', 'Region'], axis=1)
df.head()





from sklearn.cluster import KMeans


k_means = KMeans(n_clusters=2, random_state=0)
k_means.fit(df)

cluster_preds = k_means.labels_





from sklearn.metrics import calinski_harabasz_score, adjusted_rand_score





# Calinski-Harabasz Score 
ch_score = calinski_harabasz_score(df, cluster_preds)
print("Calinski-Harabasz Score:", ch_score)





# 8. Adjusted Rand score
ari_score = adjusted_rand_score(channels, cluster_preds)
print("Adjusted Rand Score:", ari_score)





from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_df = scaler.fit_transform(df)


from sklearn.cluster import KMeans

scaled_k_means = KMeans(n_clusters=2, random_state=0)
scaled_k_means.fit(scaled_df)

scaled_preds = scaled_k_means.labels_


from sklearn.metrics import adjusted_rand_score

# Adjusted Rand Index (after scaling)
scaled_ari_score = adjusted_rand_score(channels, scaled_preds)
print("Adjusted Rand Score after Scaling:", scaled_ari_score)





from sklearn.decomposition import PCA

# Instantiating PCA
pca = PCA()
pca_df = pca.fit_transform(scaled_df)


# Checking explained variance ratio
explained_variance = np.cumsum(pca.explained_variance_ratio_)
print("Cumulative Explained Variance:\n", explained_variance)


plt.plot(range(1, len(explained_variance)+1), explained_variance, marker='o')
plt.title('Cumulative Explained Variance by PCA Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()


# Assuming 4 components retain ~90% variance
pca = PCA(n_components=4)
pca_df = pca.fit_transform(scaled_df)


# Fitting KMeans to PCA-transformed data
pca_k_means = KMeans(n_clusters=2, random_state=0)
pca_k_means.fit(pca_df)

# Predictions
pca_preds = pca_k_means.labels_


# Evaluating clustering quality
final_ari = adjusted_rand_score(channels, pca_preds)
print("Adjusted Rand Score after Scaling + PCA:", final_ari)








# HAC on Raw (Unscaled) Data

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import adjusted_rand_score

# HAC on unscaled data
hac_raw = AgglomerativeClustering(n_clusters=2)
hac_raw_preds = hac_raw.fit_predict(df)

# ARI score
ari_raw = adjusted_rand_score(channels, hac_raw_preds)
print("HAC ARI (Raw Data):", ari_raw)


# HAC on scaled data
hac_scaled = AgglomerativeClustering(n_clusters=2)
hac_scaled_preds = hac_scaled.fit_predict(scaled_df)

# ARI score
ari_scaled = adjusted_rand_score(channels, hac_scaled_preds)
print("HAC ARI (Scaled Data):", ari_scaled)


# Fit PCA 
pca = PCA(n_components=2)  # You can experiment with different numbers
pca_data = pca.fit_transform(scaled_df)


# HAC on PCA-transformed data
hac_pca = AgglomerativeClustering(n_clusters=2)
hac_pca_preds = hac_pca.fit_predict(pca_data)

# ARI score
ari_pca = adjusted_rand_score(channels, hac_pca_preds)
print("HAC ARI (Scaled + PCA Data):", ari_pca)


# Results
print("\n--- Adjusted Rand Scores for HAC ---")
print(f"Unscaled Data:      {ari_raw:.4f}")
print(f"Scaled Data:        {ari_scaled:.4f}")
print(f"Scaled + PCA Data:  {ari_pca:.4f}")






